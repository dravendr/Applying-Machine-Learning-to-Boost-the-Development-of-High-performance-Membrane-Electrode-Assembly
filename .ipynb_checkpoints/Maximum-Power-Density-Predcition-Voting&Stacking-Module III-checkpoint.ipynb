{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[                     \n",
    "                       'Pt at% in metal element',#0\n",
    "                      'Co at% in metal element',#1\n",
    "                      'total metal mass ratio wt%',#2\n",
    "                      'C wt%',#3\n",
    "                      'Particle diameter （nm）',#4\n",
    "                      'support BET surface area(m2/g)' ,#5\n",
    "                      'Reduction Temperature',#6\n",
    "                      'Reduction Time/min',#7\n",
    "                      'Annealing Temperature',#8\n",
    "                      'ECSA m2/g',#9\n",
    "                      'Mass Activity mA mg-1',#10\n",
    "                      'I/C Ratio(ionomer/catalyst)',#11\n",
    "                      'Area cm2',#12\n",
    "                      'Cathodic Loading Amount mgPt cm-2',#13\n",
    "                      'Anodic Platinum Loading Amount mgPt cm-2',#14\n",
    "                      'Anodic catalyst type x wt% Pt/C',#15\n",
    "                      'membrane thickness',#16\n",
    "                      'Hot Press Temperature',#17\n",
    "                      'Hot Press Time min',#18\n",
    "                      'Hot Press Pressure Mpa',#19\n",
    "                      'Humidity %',#20\n",
    "                      'GDE for 1',#21\n",
    "                      'celltemp',#22\n",
    "                      'Flowing rate of H2 ml min-1',#23\n",
    "                      'flowing rate of cathode gas(O2/air)',#24\n",
    "                      'Back Pressure Mpa',#25\n",
    "                      'Cathode gas oxygen ratio',#26\n",
    "                      'Maximum Power Density mW cm-2'#\n",
    "                        ]]\n",
    "###########handling missing values##########\n",
    "median_raw_data=raw_data.median()\n",
    "dict_median_raw_data=median_raw_data.to_dict()\n",
    "data=raw_data.fillna(dict_median_raw_data)\n",
    "###########data standardization##########\n",
    "standardized_data = (data-np.mean(data,axis=0))/np.std(data,axis=0)\n",
    "###########train test splitting##########\n",
    "raw_param=standardized_data.iloc[:,0:27]\n",
    "raw_power=standardized_data.iloc[:,27]\n",
    "X=raw_param.values.astype(np.float32)\n",
    "y=raw_power.values.astype(np.float32)\n",
    "###########fix random seed for reproducability##########\n",
    "seed=78\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,random_state=seed)\n",
    "###########defining a wrapper function for later call from each machine learning algorithms##########\n",
    "def try_different_method(model):\n",
    "    model.fit(X_train,y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    result = model.predict(X_test)\n",
    "    x_prediction_maximum_power_ann=result*np.std(data,axis=0)[27]+np.mean(data,axis=0)[27]\n",
    "    y_real_maximum_power=y_test*np.std(data,axis=0)[27]+np.mean(data,axis=0)[27]\n",
    "    x_prediction_maximum_power_ann_series=pd.Series(x_prediction_maximum_power_ann)\n",
    "    y_real_maximum_power_series=pd.Series(y_real_maximum_power)\n",
    "    ###########evaluating the regression quality##########\n",
    "    corr_ann = round(x_prediction_maximum_power_ann_series.corr(y_real_maximum_power_series), 4)\n",
    "    rmse_val= rmse(x_prediction_maximum_power_ann,y_real_maximum_power)\n",
    "    print(rmse_val)\n",
    "    print(corr_ann)\n",
    "    print(y_real_maximum_power)\n",
    "    ###########generating a figure##########\n",
    "    x_y_x=np.arange(0,2500,100)\n",
    "    x_y_y=np.arange(0,2500,100)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_prediction_maximum_power_ann,y_real_maximum_power,color='red',label=algorithm_name)\n",
    "    ax.plot(x_y_x,x_y_y)\n",
    "    plt.legend()\n",
    "    plt.xlabel(u\"Predicted_Maximum_Power mw cm^-2\")\n",
    "    plt.ylabel(u\"Real_Maximum_Power mw cm^-2\")\n",
    "    plt.show()\n",
    "###import machine learning algorithms packages and define the corresponding models####\n",
    "####Decision Tree####\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "model_DecisionTreeRegressor = tree.DecisionTreeRegressor()\n",
    "####Linear Regression####\n",
    "from sklearn import linear_model\n",
    "model_LinearRegression = linear_model.LinearRegression()\n",
    "####Support Vector Regressor####\n",
    "from sklearn import svm\n",
    "model_SVR = svm.SVR()\n",
    "####K Nearest Neighbor####\n",
    "from sklearn import neighbors\n",
    "model_KNeighborsRegressor = neighbors.KNeighborsRegressor()\n",
    "####Random Forest####\n",
    "from sklearn import ensemble\n",
    "model_RandomForestRegressor = ensemble.RandomForestRegressor()\n",
    "####Adaboost####\n",
    "from sklearn import ensemble\n",
    "model_AdaBoostRegressor = ensemble.AdaBoostRegressor()\n",
    "####GBRT####\n",
    "from sklearn import ensemble\n",
    "model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor()\n",
    "####Bagging Regressor ####\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "model_BaggingRegressor = BaggingRegressor()\n",
    "####Extra Tree Regressor####\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "model_ExtraTreeRegressor = ExtraTreeRegressor()\n",
    "####Bayesian Ridge####\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "model_BayesianRidge =BayesianRidge()\n",
    "####Gaussian Process####\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "model_GaussianProcessRegressor=GaussianProcessRegressor()\n",
    "####SGD####\n",
    "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n",
    "model_SGDRegressor=SGDRegressor()\n",
    "####Ridge Regressor####\n",
    "from sklearn.linear_model import Ridge,RidgeCV\n",
    "model_RidgeRegressor=Ridge()\n",
    "####Kernel Ridge####\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "model_KernelRidgeRegressor=KernelRidge()\n",
    "####XGBoost####\n",
    "import xgboost as xgb\n",
    "model_XGboostRegressor=xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####CatBoost####\n",
    "import catboost\n",
    "model_CatboostRegressor=catboost.CatBoostRegressor()\n",
    "####LightGBM####\n",
    "import lightgbm\n",
    "model_LGBMRegressor=lightgbm.LGBMRegressor()\n",
    "\n",
    "####Lasso Regressor####\n",
    "from sklearn import linear_model\n",
    "model_LassoRegressor=linear_model.LassoCV()\n",
    "####Elastic Net####\n",
    "from sklearn import linear_model\n",
    "model_ElasticNetRegressor=linear_model.ElasticNetCV()\n",
    "####LARS####\n",
    "from sklearn import linear_model\n",
    "model_LARSRegressor=linear_model.LarsCV()\n",
    "####OMP####\n",
    "from sklearn import linear_model\n",
    "model_OMPRegressor=linear_model.OrthogonalMatchingPursuitCV()\n",
    "####ARD####\n",
    "from sklearn import linear_model\n",
    "model_ARDRegressor=linear_model.ARDRegression()\n",
    "####PAR####\n",
    "from sklearn import linear_model\n",
    "model_PARRegressor=linear_model.PassiveAggressiveRegressor()\n",
    "####RANSAC####\n",
    "from sklearn import linear_model\n",
    "model_RANSACRegressor=linear_model.RANSACRegressor()\n",
    "####TSR####\n",
    "from sklearn import linear_model\n",
    "model_TSRRegressor=linear_model.TheilSenRegressor()\n",
    "####Huber####\n",
    "from sklearn import linear_model\n",
    "model_HuberRegressor=linear_model.HuberRegressor()\n",
    "####Polynomial####\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "model_PolynomialRegressor=Pipeline([('poly', PolynomialFeatures(degree=5)),('linear', LinearRegression(fit_intercept=False))])\n",
    "####Linear Support Vector Regressor####\n",
    "from sklearn import svm\n",
    "model_LinearSVR = svm.LinearSVR()\n",
    "####Nu Support Vector Regressor####\n",
    "from sklearn import svm\n",
    "model_NuSVR = svm.NuSVR()\n",
    "####Voting Regressor ####\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Build all top scoring models to test them for GridSearch Voting Regressor\n",
    "xgb_estimator = xgb.XGBRegressor(learning_rate= 0.05, max_depth= 7, n_estimators= 1000, reg_alpha=0.01, reg_lambda= 0.1, subsample=0.7)\n",
    "rf_estimator = ensemble.RandomForestRegressor(max_depth=15, n_estimators=400,max_features='auto',min_samples_leaf=2, min_samples_split=2)\n",
    "ada_estimator=ensemble.AdaBoostRegressor(learning_rate=1,loss='square',n_estimators=1000)\n",
    "knn_estimator=neighbors.KNeighborsRegressor(algorithm='ball_tree',leaf_size=2,n_neighbors=8,weights='distance')\n",
    "kernel_ridge_estimator=KernelRidge(alpha=1,coef0=0.01,degree=2,gamma=0.1,kernel='rbf')\n",
    "gb_estimator=ensemble.GradientBoostingRegressor(criterion='friedman_mse',learning_rate=0.1,loss='huber',max_depth=10,max_features='auto',min_samples_split=8,n_estimators=200,subsample=0.6)\n",
    "catboost_estimator=catboost.CatBoostRegressor(learning_rate=0.1,max_depth=5,n_estimators=800,subsample=0.5,verbose=0)\n",
    "lightboost_estimator=lightgbm.LGBMRegressor(reg_alpha=0.1,reg_lambda=0.1,learning_rate=0.01,max_depth=5,n_estimators=4000,subsample=0.5)\n",
    "\n",
    "# Make list of all models\n",
    "estimators_list = [('xgb', xgb_estimator),\n",
    "              ('rf', rf_estimator),\n",
    "                  ('ada',ada_estimator),\n",
    "                  ('knn',knn_estimator),\n",
    "                  ('kernel_ridge',kernel_ridge_estimator),\n",
    "                  ('gb',gb_estimator),\n",
    "                  ('cat',catboost_estimator),\n",
    "                  ('light',lightboost_estimator)]\n",
    "model_VotingRegressor=VotingRegressor(estimators_list)\n",
    "####Stacking Regressor ####\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "model_StackingRegressor = StackingRegressor(estimators_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.32371413053806\n",
      "0.9364\n",
      "[ 671.5      243.00003  935.      1051.35     385.00003  704.\n",
      "  473.       287.00003  440.       660.       380.00003  364.00003\n",
      "  525.      1121.46    1962.8699   920.       495.00003  196.00003\n",
      "  450.00003  414.       306.00003  252.00003  333.00003  692.\n",
      "  531.       504.       705.       560.       229.80002  441.\n",
      " 1271.49     732.       287.00003  447.00003  511.00003  324.00003\n",
      "  790.       624.       574.      1187.5      174.00003 2133.24\n",
      "  690.       810.       162.00003  796.       460.       440.\n",
      "  436.80002  114.       410.       156.80002  450.00003  205.00003\n",
      "  208.00003  776.       602.       371.00003 1455.99     315.\n",
      "  756.       506.       363.424    986.49     610.41003  527.33203\n",
      "  962.       301.50003  897.       520.       540.       238.77603\n",
      " 1060.       791.46    2187.42     166.      2090.0698  1893.9299\n",
      " 2299.44     646.       195.00003  192.00003  376.00003  105.00006\n",
      "  315.      1271.28    1179.468    444.028    735.       147.99002\n",
      "  225.00003  459.      1102.5      760.       309.       611.\n",
      "  220.       828.      1502.3099   611.       620.       480.\n",
      "  616.       382.83002  450.00003  931.35     533.       300.00003\n",
      "  767.2      912.       581.      1081.35     855.       194.00003\n",
      " 1060.       360.00003  280.00003  522.5      374.00003  962.\n",
      " 1118.94    1267.8      455.00003  912.       396.00003  675.\n",
      " 1918.95     593.86     617.47205  360.00003 1288.      1240.1699\n",
      "  556.80005  405.       594.       456.      1560.       819.     ]\n"
     ]
    }
   ],
   "source": [
    "seed=2\n",
    "np.random.seed(seed)\n",
    "####Voting Regressor####\n",
    "\n",
    "algorithm_name='Voting Regressor'\n",
    "try_different_method(model_VotingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161.26407455065313\n",
      "0.9354\n",
      "[ 671.5      243.00003  935.      1051.35     385.00003  704.\n",
      "  473.       287.00003  440.       660.       380.00003  364.00003\n",
      "  525.      1121.46    1962.8699   920.       495.00003  196.00003\n",
      "  450.00003  414.       306.00003  252.00003  333.00003  692.\n",
      "  531.       504.       705.       560.       229.80002  441.\n",
      " 1271.49     732.       287.00003  447.00003  511.00003  324.00003\n",
      "  790.       624.       574.      1187.5      174.00003 2133.24\n",
      "  690.       810.       162.00003  796.       460.       440.\n",
      "  436.80002  114.       410.       156.80002  450.00003  205.00003\n",
      "  208.00003  776.       602.       371.00003 1455.99     315.\n",
      "  756.       506.       363.424    986.49     610.41003  527.33203\n",
      "  962.       301.50003  897.       520.       540.       238.77603\n",
      " 1060.       791.46    2187.42     166.      2090.0698  1893.9299\n",
      " 2299.44     646.       195.00003  192.00003  376.00003  105.00006\n",
      "  315.      1271.28    1179.468    444.028    735.       147.99002\n",
      "  225.00003  459.      1102.5      760.       309.       611.\n",
      "  220.       828.      1502.3099   611.       620.       480.\n",
      "  616.       382.83002  450.00003  931.35     533.       300.00003\n",
      "  767.2      912.       581.      1081.35     855.       194.00003\n",
      " 1060.       360.00003  280.00003  522.5      374.00003  962.\n",
      " 1118.94    1267.8      455.00003  912.       396.00003  675.\n",
      " 1918.95     593.86     617.47205  360.00003 1288.      1240.1699\n",
      "  556.80005  405.       594.       456.      1560.       819.     ]\n"
     ]
    }
   ],
   "source": [
    "seed=2\n",
    "np.random.seed(seed)\n",
    "####Stacking Regressor####\n",
    "\n",
    "algorithm_name='Stacking Regressor'\n",
    "try_different_method(model_StackingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
